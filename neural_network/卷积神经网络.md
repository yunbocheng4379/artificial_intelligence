# 卷积神经网络(CNN)

## 第一章：卷积网络介绍

### 1.1 CNN概念

- 卷积神经网络是一种专门用于处理具有网络结构数据（如图像、视频、音频等）的深度学习模型。它的核心思想是通过**局部感知、权值共享和池化操作**，有效提取输入数据的空间和时序特征。

### 1.2 CNN核心思想

- **局部感知（Local Receptive Fields）** ： 
  - 传统神经网络（如MLP）是全连接的，每个神经元都与所有的输入相连，计算量大。
  - CNN使用 **卷积核（Filter/Kernel）**在输入数据上滑动，每次只关注局部区域（如3X3或5X5的像素块），减少参数量。
- **权值共享（Share Weights）** ： 
  - 同一个卷积核在整个输入数据上 **共享参数**，使得网络能检测相同特征（如边缘、纹理）在不同位置的出现。
  - 大大减少训练参数，提高计算效率。
- **池化（Pooling）** ：
  - 用于 **降维**，减少计算量并增强特征的平移不变性。
  - 常见方法： **最大池化（Max Pooling）、平均池化（Average Pooling）**。
- **平移不变性**
  - 无论特征出现在图像哪个位置，都能被检测到。

### 1.3 CNN基本结构

CNN 通常由以下层组成：

| **层类型**         | **作用**                                   |
| --------------- | ---------------------------------------- |
| **输入层（Input）**  | 接收原始数据（如 224×224×3 的RGB图像）。              |
| **卷积层（Conv）**   | 使用多个卷积核提取特征（如边缘、颜色、纹理）。                  |
| **激活层（ReLU）**   | 引入非线性（如 ReLU、Sigmoid、LeakyReLU），增强模型表达能力。 |
| **池化层（Pool）**   | 降维（如 2×2 Max Pooling 使特征图尺寸减半）。          |
| **全连接层（FC）**    | 在最后几层使用，用于分类或回归（如输出类别概率）。                |
| **输出层（Output）** | 根据任务输出结果（如分类用 Softmax，回归用线性输出）。          |

**典型CNN 架构示例（如LeNet-5[最早]、AlexNet[深度学习复兴]、ResNet[残差连接]）**：

```
输入[Input] → (卷积层[Conv] → 激活函数[如ReLU] → 池化层[Pooling])×N → 平铺[Flatten] → 全连接层[FC] → 输出[Output](全连接 + softmax激活函数)
```

![](https://img-1259707064.cos.ap-guangzhou.myqcloud.com/202505061931915.png)



### 1.4 CNN核心操作

- 使用 **卷积核(滤波器)** 滑动扫描输入数据（如图像），提取局部特征（如边缘、纹理）。
  $$
  输出特征图 = 输入 * 卷积核 +  偏置
  $$


### 1.5 CNN经典应用

- **图像分类（Image Classification）** ：ResNet、VGC、EfficientNet。
- **目标检测（Object Detection）** ：YOLO、Faster R-CNN。
- **语义分割（Semantic Segmentation）** ：U-Net、DeepLab。
- **人脸识别（Face Recognition）** ：FactNet、ArcFace。
- **医学影像分析（Medical Imaging）** ：检测肿瘤、X光分析。

此外，CNN也适用于：

- **自然语言处理（NLP）** ：文本分类（TextCNN）。
- **时序数据分析** ： 1D-CNN处理股票预测、语音识别。



## 第二章：卷积层涉及参数

### 2.1 核心参数分类

- 卷积层参数分为 **超参数（手动设定）** 和 **可学习参数（自动优化）** 两类：

  | 类型        | 参数          | 作用            | 是否可训练 |
  | --------- | ----------- | ------------- | ----- |
  | **超参数**   | 卷积核尺寸、步长、填充 | 控制特征提取方式和输出尺寸 | 否     |
  | **可学习参数** | 卷积核权重、偏置项   | 自动学习输入数据的特征表示 | 是     |

### 2.2 超参数详解

**1. 卷积核尺寸（Kernel Size）**

- **定义**： 卷积核的宽度和高度（如3X3）.
- **作用** ：决定局部感受野大小，影响特征提取的粒度。
  - 小尺寸（如1X1）：用于通道混合或降维。
  - 中尺寸（3X3）：平衡感受野和计算效率（最常用）。
  - 大尺寸（5X5或7X7）：捕捉更大范围的空间特征（深层网络较少用）。
- **公式** ：输出尺寸与核尺寸成反比（见下文）。

**2. 步长（Stride）**

- **定义** ：卷积核滑动的步长（如步长=2，每次移动2像素）。

- **作用** ：控制输出特征图的分辨率。

  - 步长=1 ： 输出尺寸最大，保留细节。
  - 步长>=2 ：降采样，减少计算量，扩大感受野。

- **公式** ：输出尺寸与步长成反比：
  $$
  W_{out} = [\frac{W_{in} + 2P-K}{S}] + 1
  $$
  其中：

  - $W_{in}$ :  输入数宽度。
  - $P$ : 填充数。
  - $K$ : 卷积核尺寸。
  - **S** ：步长。

**3. 填充（Padding）**

- **定义** ： 在输入周围额外添加的像素（通常填充0，0表示无特征，不会对结果造成影响，只是为了占位。）

- **作用** ：控制输出尺寸并保留边缘信息。

  - **Valid（无填充）** ： 输出尺寸缩小。

  - **Same（填充至输出尺寸 = 输入尺寸）** ：
    $$
    P = [\frac{(K-1)}{2}]  (如K=3 ⇒ P=1)
    $$


**4. 输出通道数（Filters/Channels）**

- **定义** ：卷积层生成的独立特征图数量。
- **作用** ：每个通道对应一个卷积核，提取不同类型的特征。
  - 通道数少（如16） ：低级特征（边缘、纹理）。
  - 通道数多（如256）： 高级语义特征（物体部件）。

### 2.3 可学习参数详解

**1. 卷积核权重（Weights）** 

- **形状** ：$K * K * C_{in} * C_{out}$ 

  - $K * K$ : 卷积核尺寸
  - $C_{in}$ : 输入通道数
  - $C_{out}$ : 输出通道数

- **参数量**  ：
  $$
  参数量 = K^2  * C_{in} * C_{out}
  $$
  例如：3*3的卷积核，输入64通道，输出128通道  → 32×64×128=73,72832×64×128=73,728 参数。

**2. 偏置项（Bias）**

- **形状** ：$C_{out} * 1$
- **作用** ：为每个输出通道增加偏移量（可选参数）。

### 2.3 关键参数说明

| 参数              | 作用                        | 典型值        |
| --------------- | ------------------------- | ---------- |
| **卷积核大小**       | 决定感受野大小，影响特征提取粒度          | 3×3, 5×5   |
| **步长（Stride）**  | 控制滑动步长，步长越大输出尺寸越小         | 1, 2       |
| **填充（Padding）** | 保持输出尺寸与输入一致（如 "same" 填充）  | 0（不填充）     |
| **输出通道数**       | 决定生成多少种不同的特征（每个通道对应一个卷积核） | 16, 32, 64 |

![](https://img-1259707064.cos.ap-guangzhou.myqcloud.com/202505061931059.png)

### 2.4 参数对性能的影响

| 参数         | 计算量   | 内存占用 | 模型表现            |
| ---------- | ----- | ---- | --------------- |
| **卷积核尺寸↑** | 平方级增长 | 显著增加 | 捕捉更大范围特征，但可能过拟合 |
| **步长↑**    | 大幅降低  | 降低   | 丢失细节，但加速训练      |
| **通道数↑**   | 线性增长  | 线性增长 | 增强模型表达能力，可能过拟合  |

### 2.5 实际调参策略

1. **轻量化设计** ：使用 1X1 卷积降维（如MobileNet）。
2. **深度网络** ：堆叠小卷积核（如3X3）代替大核，减少参数且增加非线性（VGGNet）。
3. **分辨率保留** ： 步长=1+Same填充（用于分割任务，如U-Net）。
4. **动态感受野** ：空洞卷积扩大感受野而不增加参数。



## 第三章：卷积结果计算公式

### 3.1 输出特征图尺寸计算公式

输出尺寸由以下公式计算：
$$
H_{out}  = [\frac{H_{in}+2P-K}{S}]+1
$$

$$
W_{out}  = [\frac{W_{in}+2P-K}{S}]+1
$$

其中：

- $H_{in}$ :  输入数据长度。
- $W_{in}$ : 输入数据宽度。
- $P$ : 边缘填充大小。
- $K$ : 卷积核大小。
- $S$ : 步长。

### 3.2 计算示例

**示例1：单通道输入，无填充，步长=1**

- **输入（2×2）**：
  $$
  \begin{bmatrix} 1 & 3 \\ 2 & 4 \end{bmatrix}
  $$



- **卷积核（2×2）**：
  $$
  \begin{bmatrix} 0 & 1 \\ 2 & 3 \end{bmatrix}
  $$



- **输出计算** ：
  1. 左上位置：(1×0)+(2×1)+(3×2)+(4×3)=0+2+6+12=20。
  2. 由于输入和卷积核均为2×2，输出尺寸为1×1，结果为标量 **20**。

#### 示例2：多通道输入（RGB图像）

- **输入**：3通道（RGB），每通道3×3
- **卷积核**：2×2×3（深度与输入通道一致）
- **计算过程** ：
  1. 卷积核在三个通道上分别滑动计算，结果相加：最终输出为一个通道的特征图。

$$
Output(i,j,k) =  \sum_{c=1}^{3}(Input_c * Kernel_c) + b
$$

## 第四章：卷积层的原理及计算过程

![](https://img-1259707064.cos.ap-guangzhou.myqcloud.com/202505061931438.png)

### 2.1 核心原理

- **局部连接** ： 每个神经元仅连接输入数据的局部数据（而非全连接）。
- **参数共享** ：同一个卷积核在整个输入上滑动使用，大幅减少参数量。
- **平移不变性** ：无论特征出现在输入哪个位置，都能被检测到。

### 2.2 卷积运算的数学过程

**1. 输入与卷积核定义**

- **输入（Input）** ：尺寸为 $H_{in} * W_{in} * C_{in}（高 * 宽 * 通道数）$
- **卷积核（Kernel）** ： 尺寸为 $K * K * C_{in} * C_{out} (高 * 宽 * 输入通道数 * 输出通道数[卷积核个数])$

**注意：卷积核的通道数必须和输入的数据通道数一致，也就是输入通道数，卷积核的个数可以自定义。**

**2. 单次卷积计算**

对输入的第c一个通道（$1 <= c <= C_{in}$），卷积核在位置 $(i,j)$ 处的计算为：
$$
Output(i,j,k) =  \sum_{m=0}^{K-1}\sum_{n=0}^{K-1}\sum_{c=0}^{C_{m}}Input(i+m,j+n,c) · Kernel(m,n,c,k) + b_{k}
$$
其中：

- $K$ ：卷积核大小（如3X3）。
- $b_{k}$ : 第$k$个输出通道的偏置项。



## 第五章：卷积层参数共享

### 5.1 定义与原理

参数共享是指在整个输入数据的不同位置上，**重复使用同一卷积核的权重**。

- **核心思想** ： 无论目标特征（如边缘、纹理）出现在图像的哪个位置，都使用相同的卷积核来检测它。
- **类比** ： 相当于用一个“模板”在图像上滑动，检查每个位置是否存在该模板对应的特征。

总结：参数共享是CNN高效性的核心设计，它通过复用卷积核权重显著降低模型复杂度，同时赋予模型平移不变性，这种机制使得CNN能够以少量参数处大规模图像数据，成为计算机视觉任务的基石。

### 5.2 核心优势

| 优势        | 说明                                       |
| --------- | ---------------------------------------- |
| **减少参数量** | 全连接层的参数量随输入尺寸平方增长，而卷积层仅与卷积核尺寸相关（见下表对比）。  |
| **平移不变性** | 同一特征在不同位置被同一卷积核检测，模型对位置变化更鲁棒（如猫在左/右侧都能识别）。 |
| **计算高效**  | 共享参数减少计算量，适合处理高分辨率图像。                    |

### 5.2 与全连接层的对比

假设输入为 224×224×32 的图像，输出维度为 64：

- **全连接层参数量**：
  $$
  (224×224×3)×64=9,633, ⁣792
  $$



- **卷积层参数量**（3×3卷积核）：
  $$
  (3×3×3)×64=1, ⁣728
  $$


**对比结果**：卷积层参数量仅为全连接的约 **0.018%**。

### 5.3 实际示例

- **边缘检测** ： 一个水平梯度卷积核 $\begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}$ 在整个图像上滑动，检测所有位置的垂直边缘。
- **特征泛化** ：无论边缘出现在左上角或右下角，同一卷积核均能检测，无需为买个位置学习独立参数。

### 5.4 参数共享的扩展理解

- **平移等边性（Translation Equivariance）** ： 输入平移 → 输出也相应平移。如，图像中的猫向右移10像素，特征图中的相应也右移10像素。
- **层次结构弥补感受野** ：深层卷积层通过堆叠小卷积核（如3X3），以少量参数捕获更大的感受野（类似5X5或7X7的效果）。



## 第六章：卷积层边界填充

### 6.1 填充的目的

边界填充通过在输入数据的周围添加额外的像素（通常为0），解决以下问题：

1. **边界信息丢失** ： 卷积核在图像边缘无法覆盖整个区域，导致边缘特征提取不充分。
2. **输出尺寸缩小** ： 无填充时，输出尺寸会随着卷积层数增加而快速减小（尤其深层网络）。
3. **保持空间分辨率** ：填充后输出尺寸与输入一致，适合需要高分辨率输出的任务（如图像分割）。

### 6.2 填充类型

常见的填充模式有两种：

| 填充模式            | 公式与行为             | 输出尺寸示例（输入=5×5，核=3×3，步长=1） |
| --------------- | ----------------- | ------------------------- |
| **Valid（无填充）**  | 不添加任何像素，输出尺寸自然缩小。 | 输出尺寸=3×3                  |
| **Same（等尺寸填充）** | 填充使得输出尺寸与输入尺寸相同。  | 输出尺寸=5×5                  |

### 6.3 填充公式

输出尺寸由以下公式计算：
$$
输出宽度 = [\frac{W_{in}+2P-K}{S}]+1
$$

1. **Vaild填充**

   填充量：P=0，自然缩小，由公式直接计算。

2. ##### **Same 填充**

   -  **目标** ： $输出尺寸 W_{out} = W_{in}$

   - **填充量计算** ：

     通过公式反推P：
     $$
     P=[\frac{(W_{in}-1)*S-W_{in}+K}{2}]
     $$
     当 S = 1 时，简化为：
     $$
     P=[\frac{K-1}{2}]
     $$
     例如：$K=3⇒P=1, 𝐾=5⇒𝑃=2$

### 6.4 具体示例

**示例1：Valid填充**

- 输入尺寸：5×5

- 卷积核：3×3，步长=1

- 输出尺寸：
  $$
  W_{out}=[\frac{5+0-3}{1}] + 1 = 3
  $$
  输出为 3x3.

**示例2：Same填充**

- 输入尺寸：5×5

- 卷积核：3×3，步长=1

- 填充量计算：
  $$
  P=[\frac{3-1}{2}]=1
  $$
  输出尺寸：
  $$
  W_{out}=[\frac{5+2*1-3}{1}]+1=5
  $$
  输出仍为5X5。

### 6.5 不同填充策略对比

| 场景        | Valid填充        | Same填充         |
| --------- | -------------- | -------------- |
| **目标**    | 减少计算量，允许输出尺寸缩小 | 保持分辨率，避免信息丢失   |
| **适用任务**  | 分类任务（最终需要全局特征） | 分割、检测（需空间精度）   |
| **参数量影响** | 无              | 无（填充的0不参与参数更新） |



## 第七章：卷积层实际应用

### 7.1 卷积层在实际领域的应用

#### **1. 图像分类（Image Classification）**

- **任务**：识别图像中的物体类别（如猫、狗、汽车）。
- **应用**：经典模型如 **AlexNet**、**VGGNet**、**ResNet**。
- **卷积层作用** ：
  - 浅层卷积：提取边缘、颜色、纹理等低级特征。
  - 深层卷积：组合低级特征，检测物体部件（如车轮、眼睛）。
- **案例**：ImageNet 竞赛中，ResNet-50 使用堆叠的 3×3 卷积层，实现 76% 的 Top-1 准确率。

#### **2. 目标检测（Object Detection）**

- **任务**：定位图像中多个物体并分类（如 YOLO、Faster R-CNN）。
- **卷积层作用** ：
  - **骨干网络（Backbone）**：提取多尺度特征（如 ResNet、MobileNet）。
  - **特征金字塔（FPN）**：融合不同层级的特征，提升小物体检测能力。
- **案例**：YOLOv5 使用 **CSPDarknet** 骨干网络，通过跨阶段卷积减少计算量，实现实时检测。

#### **3. 语义分割（Semantic Segmentation）**

- **任务**：为图像每个像素分配类别标签（如区分道路、行人、车辆）。
- **卷积层变体** ：
  - **空洞卷积（Dilated Conv）**：扩大感受野，避免降采样丢失细节（如 DeepLab）。
  - **转置卷积（Transposed Conv）**：上采样恢复分辨率（如 U-Net 的解码器）。
- **案例**：U-Net 的对称编码器-解码器结构，通过跳跃连接保留细节，广泛用于医学图像分割。

#### **4. 音频处理（Audio Processing）**

- **任务**：语音识别、音乐生成。
- **卷积层作用** ：
  - 2D 卷积：处理音频的时频图（如梅尔频谱）。
  - 1D 卷积：直接处理波形信号，提取时序特征。
- **案例**：WaveNet 使用空洞卷积生成高质量语音，模拟人类发声。

#### **5. 视频分析（Video Analysis）**

- **任务**：行为识别、视频摘要。
- **卷积层变体** ：
  - **3D 卷积**：同时处理时空维度（如 C3D 网络）。
  - **时序分离**：2D 卷积处理空间 + 1D 卷积处理时间（如 I3D 网络）。

#### **6. 医疗影像（Medical Imaging）**

- **任务**：肿瘤检测、器官分割。
- **卷积层作用** ：
  - **小样本学习**：通过预训练卷积层迁移特征（如 CheXNet 检测肺炎）。
  - **高分辨率保持**：U-Net 的 Same 填充 + 跳跃连接，精确标注病变区域。

### 7.2 卷积层在经典模型中的应用

| 模型                          | 核心卷积设计                   | 应用场景                |
| --------------------------- | ------------------------ | ------------------- |
| **ResNet**                  | 残差连接 + 3×3 卷积堆叠          | 图像分类、目标检测           |
| **EfficientNet**            | 复合缩放（深度/宽度/分辨率） + MBConv | 移动端高效推理             |
| **ViT（Vision Transformer）** | 卷积嵌入层（替代图像分块）            | 图像分类（结合Transformer） |
| **Mask R-CNN**              | ROI Align + 卷积掩模分支       | 实例分割                |

## 第八章：池化层原理

### 8.1 核心目的

池化层（Pooling Layer）: 的主要作用是**降维**和**特征不变性增强**，通过减少特征图的空间尺寸，实现以下目标：

- **降低计算复杂度** ： 减少后续层的参数和计算量。
- **扩大感受野** ：使深层网络能够捕获更广域的上下文信息。
- **增强平移\旋转不变性** ：对输入的小范围变化不敏感，提升模型鲁棒性。

池化层通过局部区域的极值或平均操作，实现了特征降维、计算优化和模型鲁棒性提升。

### 8.2 池化的类型

##### **(1) 最大池化（Max Pooling）**

- **操作**：在滑动窗口内取最大值。

- **公式**：对输入区域 𝑅 中的元素，输出为：
  $$
   \text{输出} = \max\limits_{i,j \in \mathbb{R}} x_{i,j}
  $$

- **特点**：保留最显著特征（如边缘、纹理），适用于多数CV任务。

##### **(2) 平均池化（Average Pooling）**

- **操作**：在滑动窗口内取平均值。

- **公式**：
  $$
  \text{输出} = \frac{1}{|R|} \sum\limits_{i,j \in R} x_{i,j}
  $$


##### **(3) 示例**

- **输入矩阵**（4×4）：
  $$
  \begin{bmatrix} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 10 & 11 & 12 \\ 13 & 14 & 15 & 16 \end{bmatrix}
  $$



- 池化窗口（2×2，步长=2）：

  ![](https://img-1259707064.cos.ap-guangzhou.myqcloud.com/202505061837453.png)

  - **最大池化输出（找出最大）**（2×2）：
    $$
    \begin{bmatrix} 6 & 8 \\ 14 & 16 \end{bmatrix}
    $$

  - **平均池化输出（计算平均值）**（2×2）：
    $$
    \begin{bmatrix} 3.5 & 5.5 \\ 11.5 & 13,5 \end{bmatrix}
    $$


### 8.3 池化层的特征

| **特性**     | **说明**                                   |
| ---------- | ---------------------------------------- |
| **无参数**    | 池化操作是固定的，无需学习权重（与卷积层不同）。                 |
| **局部连接**   | 仅对窗口内元素操作，类似卷积的局部感受野。                    |
| **反向传播机制** | - **最大池化**：梯度仅传递到前向传播中最大值的位置。- **平均池化**：梯度均分到窗口内所有位置。 |

### 8.4 池化层的实际应用

##### **(1) 降维与计算优化**

- 假设输入为224×224×64的特征图，经过 2×2 最大池化（步长=2）后：
  - 输出尺寸：112×112×64112×112×64，数据量减少 **75%**。
  - 后续卷积层的计算量降低为原来的 **25%**。

##### **(2) 平移不变性**

- **示例**：图像中的猫轻微平移后，池化层仍能提取相同特征（如猫眼的最大响应值不变）。

##### **(3) 扩大感受野扩展**

- 池化层堆叠后，深层网络的感受野指数级增长：
  - 输入图像 → 卷积层（3×3）→ 池化层（2×2）→ 下一层卷积（3×3）
  - 最终感受野：3+(2×3)=9×93+(2×3)=9×9。

### 8.5 池化层的局限性

- **信息丢失**：丢弃非极值区域细节，可能影响精确定位（如物体边缘模糊）。
- **固定模式**：无参数学习，可能无法自适应数据分布。
- **替代方案兴起**：在部分任务中，步长卷积或空洞卷积表现更优。

### 8.6 池化层的替代方案

| **方法**                 | **原理**                | **适用场景**             |
| ---------------------- | --------------------- | -------------------- |
| **步长卷积（Strided Conv）** | 使用步长 >1 的卷积直接降维，替代池化。 | 需要保留更多位置信息的任务（如分割）   |
| **空洞卷积（Dilated Conv）** | 通过间隔采样扩大感受野，避免降采样。    | 语义分割（如 DeepLab）      |
| **空间金字塔池化（SPP）**       | 多尺度池化融合不同分辨率特征。       | 目标检测（如 Faster R-CNN） |



## 第九章：多层卷积的作用

- 多层卷积通过堆叠多个卷积层，逐步提取和组合特征，实现从局部到全局的语义抽象。其核心价值体现在以下方面：

  | **作用**        | **原理说明**                                 | **实际意义**                          |
  | ------------- | ---------------------------------------- | --------------------------------- |
  | **局部特征逐级提取**  | 浅层卷积捕捉低级特征（边缘、纹理），深层卷积组合低级特征为高级语义（物体部件、整体结构）。 | 模拟人类视觉系统的层次化处理机制。                 |
  | **感受野指数级扩大**  | 每层卷积通过叠加扩大感受野。例如：3层3×3卷积等效于1层7×7卷积的感受野，但参数更少。 | 深层网络可捕获更大范围的上下文信息，提升分类、检测等任务的准确性。 |
  | **非线性表达能力增强** | 每层卷积后接激活函数（如ReLU），多层堆叠后整体非线性拟合能力显著提升。    | 解决复杂数据分布的建模问题（如细粒度分类）。            |
  | **参数效率优化**    | 多层小卷积核（如3×3）替代单层大卷积核（如7×7），减少参数量同时增加网络深度。 | 在保持性能的同时降低过拟合风险（经典案例：VGGNet）。     |
  | **跨层特征融合**    | 残差连接（ResNet）、密集连接（DenseNet）等结构实现跨层特征复用。  | 缓解梯度消失问题，提升训练效率和模型泛化能力。           |

- 层级对应的特征

  | **网络深度**     | **特征类型**       | **可视化示例**   | **任务关联性**        |
  | ------------ | -------------- | ----------- | ---------------- |
  | **浅层（1~3层）** | 边缘、颜色、纹理       | 梯度方向、斑点、条纹  | 通用特征，适用于预训练迁移学习  |
  | **中层（4~6层）** | 物体部件（车轮、眼睛、门窗） | 圆形、矩形、组合形状  | 细粒度分类、目标检测的关键层级  |
  | **深层（7+层）**  | 整体结构（车辆、人脸、建筑） | 复杂对象轮廓、场景布局 | 图像分类、语义分割的语义理解核心 |



## 第十章：传统神经网络(ANN)和卷积神经网络(CNN)的区别

| 对比维度       | 传统神经网络（ANN/MLP）                    | 卷积神经网络（CNN）                        |
| ---------- | ---------------------------------- | ---------------------------------- |
| **输入数据**   | 要求数据是 **一维向量**（需平展，如 [784] 的MNIST） | 直接处理**多维网格数据**（如图像[H,W,C]、音频[T、F]） |
| **连接方式**   | **全连接**（每个神经元连接所有输入）               | **局部连接**（卷积核只扫描局部区域）               |
| **参数共享**   | 无共享，每个连接有独立权重                      | **权值共享**（同一个卷积核在不同位置复用）            |
| **空间信息处理** | 忽略数据的空间结构（展平后丢失位置信息）               | **保留空间关系**（通过滑动窗口提取局部特征）           |
| **参数量**    | 巨大（易过拟合，如 1000X1000图像需要） 10^6 权重   | 较少（因权值共享和局部连接，如3x3卷积核只需要9个权重）      |
| **典型案例**   | 简单分类/回归（如房价预测、手写数据识别）              | 图像/视频处理（分类、检测、分割）、时序数据（1D-CNN）     |
| **特征提取**   | 需手动设计特征或依赖全连接层学习                   | **自动学习层次化特征**（边缘 → 纹理 → 物体部件）      |
| **计算效率**   | 低（高维数据计算成本高）                       | 高（卷积操作可并行优化）                       |
| **平移不变性**  | 无（输入变化导致输出剧烈变化）                    | 有                                  |
| **代表模型**   | 多层感知机（MLP）                         | LeNet、AlexNet、ResNet、YOLO          |



- **参数共享** ：减少计算量，防止过拟合。
- **自动特征提取** ：无需手动设计特征。
- **平移不变性** ： 