# Transformer

## 第一章 Transformer介绍

### 1.1 Transformer概念

- Transformer是一种基于**自注意力机制（Self-Attention）**的深度学习模型架构，最初由Google在2017年的论文《Attention Is All You Need》中提出。它被设计用于处理序列数据（如文本、语音、时间序列等），**摆脱了传统循环神经网络（RNN）和卷积神经网络（CNN）的局限性，**通过并行化和全局依赖建模能力，成为自然语言处理（NLP）和计算机视觉（CV）领域的核心架构。

### 1.2 本质

Transformer的本质是通过**自注意力机制（Self-Attention）**和**位置编码（Positional Encoding）**实现对序列数据的建模，解决了RNN两个核心问题：

- **长距离依赖** ：传统RNN难以捕捉长序列中远近距离的关系，而Transformer通过注意力机制直接建模全局依赖。
- **并行化能力** ：RNN必须顺序列处理序列，Transformer则通过矩阵运算实现并行计算，极大提升训练效率。

**关键思想** ： 将输入序列中的每个元素（如单词）与其他所有元素进行交互，动态分配权重，从而捕获更加复杂的关系。

### 1.3 作用

- **序列建模** ：广泛应用于机器翻译、文本生成、语音识别等任务。
- **预训练模型基础** ：催生了BERT、GPT、T5等大规模训练模型，推送NLP进入“预训练+微调”时代。
- **跨领域通用性** ：从NLP扩展到CV（如Vision Transformer）、多模态（如图文生成模型DALL-E）、强化学习等领域。
- **高效推理** ：通过并行计算和注意力机制优化，处理长文本或高维数据时更高效。

### 1.4 核心组件

Transformer的核心技术包括：

1. **自注意力机制（Self-Attention）** : 
   - 计算每个元素与其他元素的关联权重（Query-Key-Value机制）。
   - **多头注意力**：并行多组注意力头，捕捉不同维度的语义关系。
2. **位置编码（Positional Encoding）** ：
   - 通过正弦函数或可学习参数，为序列中的元素添加位置信息。
3. **编码器-解码器架构** ：
   - **编码器** ：堆叠多层自注意力+前馈神经网络（FFN），提取特征。
   - **解码器** ：在自注意力基础上增加交叉注意力（Cross-Attention），用于生成任务。
4. **残差连接与层归一化** ：缓解梯度消失，加速模型收敛。
5. **前馈神经网络（FFN）** ：对注意力输出进行非线性变换。

![](https://img-1259707064.cos.ap-guangzhou.myqcloud.com/202505121641236.png)

## 第二章 核心组件

### 2.1 自注意力机制（Self-Attention）

- **作用** ：捕捉序列中任意两个元素之间的全局依赖关系，替代RNN的时序依赖。

- **核心公式**：
  $$
  \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right)V
  $$

  - **$Query（Q）、Key（K）、Value（V）$** :  通过输入向量与权重矩阵乘积得到，分别表示 “要查询什么”、 
    ”匹配的键是什么“、”对应的值是什么“。
  - **缩放点积** ：通过除以$\sqrt{d_k}(d_k)是Key的维度$防梯度爆炸。
  - **Softmax** : 将注意力权重归一化为概率分布，表示每个元素的重要性。

- **关键特征** :

  - **并行计算** ：所有位置的注意力权重可同时计算。
  - **多头注意力（Multi-Head Attention）** :  将Q/K/V拆分为多个子空间（如8个头），每个头独立学习不同维度的语义关系，最后拼接结果。例如：一个头关注语法结构，一个头关注指代关系。

### 2.2 位置编码（Positional Encoding）

- **作用** ：为序列中的每个位置添加位置信息，弥补自注意力机制对顺序不敏感的问题。

- **核心公式** ：

  - **正弦波函数（原始论文方法）**
    $$
    PE_{(pos,2i)} = sin(\frac{pos}{10000^{2i/d}})
    $$

    $$
    PE_{pos,2i+1} = cos{\frac{pos}{10000^{2i/d}}}
    $$

  - $pos$ : 位置序号，$i$ : 维度序号，$d$ : 模型维度。

  - 通过不同频率的正弦/余弦函数，使模型能区分位置远近。

  - **可学习的位置编码（如BERT）** :  直接训练一个位置嵌入矩阵。

  **本质** ： 将位置信息编码为向量，与输入词向量相加，使模型感知顺序。

### 2.3 编码器-解码器架构（Encoder-Decoder）

#### 2.3.1 编码器（Encoder）

- **结构** ： 由$N$个相同的层堆叠（如6层），没层包含：
  1. **多头自注意力层** （关注输入序列内部关系）。
  2. **，前馈神经网络（FFN）** （对注意力结果进行非线性变换）。
  3. **残差连接+层归一化** （在每个子层后应用）。
- **功能** ：将输入序列映射为高级语义表示（将传统文本映射为矩阵）。

#### 2.3.2 解码器（Decoder）

- **结构** ： 同样由$N$个层堆叠，每层包含：
  1. **掩码多头子注意力层** （防止当前位置关注未来信息，用于自回归生成）。
  2. **交叉注意力层** （关注编码器输出，对其输入与输出）。
  3. **前馈神经网络（FFN）**。
  4. **残差连接+层归一化**。
- **功能** ：根据编码器输出和已生成的部分结果，逐步预测下一个元素（如机器翻译中的目标词）。

### 2.4 残差连接（Residual Connection）与层归一化（Layer Normalization）

- **作用** ： 解决深层网络训练中的梯度消失/爆炸问题，加速收敛。

- **残差连接** ： 将子层（如自注意力或FFN）的输入直接加到其输出上：
  $$
  Output = LayerNorm(x+Sublayer(x))
  $$
  保留原始信息，使梯度更容易回传。

- **层归一化** ：对每个样本的所有特征维度进行归一化（而非批归一化），稳定训练过程。

**执行顺序** ：输入 → 子层计算 → 残差相加 → 层归一化。

### 2.5 前馈神经网络（Feed-Forward Network，FFN）

- **作用** ：对自注意力层的输出进行非线性变换，增强模型表达能力。

- **结构** ：每个FFN包含两个线性层和激活函数：
  $$
  FFN(x) = ReLU(xW_1+b_1)W_2+b_2
  $$

  - **第一层** ：将维度放大（如 512 → 2048），通过ReLU引入非线性。
  - **第二层** ：将维度恢复原始大小（2048 → 512）。

- **本质** ：相当于”感知机“，对每个位置的表示独立变换。



## 第三章 组件间协同工作

### 3.1 工作流程

- **输入处理**
  - **词嵌入（Input Embedding）** ：将输入的原始词转换为对应的向量表示，这个过程称为词嵌入。例如：”apple“会被映射为一个高维向量。
  - **位置编码（Positional Encoding）** : 由于Transformer本身没有位置信息感知能力，需要额外添加位置拜尼马来保留词序信息。
- **编码器（Encoder）部分**
  - **多头注意力机制（Multi-Head Attention）** ： 并行计算多个注意力头，捕捉不同子空间的语义信息。
  - **前馈网络（Feed Forward）** :  对特征进行非线性变换。
  - **残差连接与层归一化（Add & Norm）** :  每个子层后都会进行残差连接和层归一化操作。
- **解码器（Decoder）部分**
  - **掩码多头注意力（Masked Multi-Head Attention）** :  防止解码时看到未来信息。
  - **交叉注意力（Cross Attention）** ：连接编码器和解码器的关键机制。
  - **输出处理** ： 最后通过全连接层和Softmax得到输出概率分布。

![](https://img-1259707064.cos.ap-guangzhou.myqcloud.com/202505121641237.png)



## 第四章 多模态发展趋势

### 4.1 核心思想

- **特征聚合** ： 通过注意力机制整合不同模态（图像、点位、文本、语音等）的特征信息。
- **应用场景** ：如自动驾驶需要同时处理摄像头、雷达、GPS等多源数据。

### 4.2 实现方式

- **跨模态注意力** ：让模型自动学习不同模态间的重要关联。
- **类比解释** ：就像过年走亲戚，从各家挑选最后价值的东西带回来。




## 第五章 Transformer在CV领域的应用

- 以上都是讲解Transformer在NLP（自然语言处理）方面的工作流程，因为输入值都是一段文本信息，寻找的都是输入文本的每个词语之间在整个全局特征中的占比值。

- **Transformer在CV领域的架构**

  ![](https://img-1259707064.cos.ap-guangzhou.myqcloud.com/202505121817182.png)

  **关键点** : 在输入数据处理过程中，图片不能像纯文本一样可以直接通过**词嵌入**转换为特征向量，需要先将 图片切割 → CNN（提取特征向量） →  Transformer处理。

  ![](https://img-1259707064.cos.ap-guangzhou.myqcloud.com/202505121822730.png)

  ​

  ![](https://img-1259707064.cos.ap-guangzhou.myqcloud.com/202505121821184.png)